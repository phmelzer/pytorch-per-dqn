{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4664d535",
   "metadata": {},
   "source": [
    "# Prioritized Experience Replay (PER)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f4852c4",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "- [1 - Motivation](#1)\n",
    "- [2 - Prioritized Experience Replay](#2)\n",
    "\n",
    "\n",
    "Full paper: [Prioritized experience replay (2016)](https://arxiv.org/pdf/1511.05952.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c680199c",
   "metadata": {},
   "source": [
    "<a name='1'></a>\n",
    "# 1 - Motivation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48b11779",
   "metadata": {},
   "source": [
    "Online Reinforcement Learning agents incrementally update their parameters (of the policy, value function or model) while they observe a stream of experience. In their simplest form, they discard incoming data immediately, after a single update. Two issues with this are (a) strongly correlated updates that break the i.i.d. assumption of many popular stochastic gradient-based algorithms, and (b) the rapid forgetting of possibly rare experiences that would be useful later on. \n",
    "\n",
    "Experience Replay addresses both of these issues: with experience stored in a replay memory, it becomes possible to break the temporal correlations by mixing more and less recent experience for the updates, and rare experience will be used for more than just a single update. In general, experience replay can reduce the amount of experience required to learn, and replace it with more computation and more memory - which are often cheaper resources then the RL agent's interactions with its environment.\n",
    "\n",
    "Which the original experience replay technique we have a fixed length buffer, where we store each timesteps experience tuple so that we can randomly sample a batch of tuples to train the agent at the same time. But there is one main improvement that we can make and and this is got to do with how we sample experiences to train from the buffer. The key idea behind prioritized experience replay is to prioritize experiences, that the agent can learn more from than from others. For example an action that causes the environement to terminate. The aim is to make the most effective use of the replay memory for learning, assuming that its contents are outside of our control.\n",
    "\n",
    "Experience replay liberates online learning agents from processing transitions in the exact order they are experienced. Prioritized replay further liberates agents from considering transitions with the same frequency that they are experienced."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca5bd797",
   "metadata": {},
   "source": [
    "<a name='2'></a>\n",
    "# 2 - Prioritized Experience Replay"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a348fd97",
   "metadata": {},
   "source": [
    "**Prioritizing with TD-Error**\n",
    "\n",
    "A central component of prioritized experience replay is the criterion by which the importance of each transistion is measured. The transition's TD error $\\delta$ is a reasonable proxy of the the amount the RL agent can learn from a transition in its current state (expected learning progress). This criterion is particulararly suitable for incremental, online RL algorithms, such as SARSA or Q-Learning. These algorithmns already compute the TD error and update their parameters in proportion to $\\delta$, which is the difference between the predicted q-value for the state action pair and the q-target value calculated from the bellman equation:\n",
    "\n",
    "$$\\delta = q(s, a) - q_{target}$$\n",
    "\n",
    "There is just one small issue here, that is when the error turns out to be zero. We don't want our priority weight to be zero as this experience might be useful later on. So we add a small offset-value $\\epsilon$ to each priority to prevent the edge-case of transitions not being revisited once their error is zero. Now we can actually set our priority value for each tuple to be directly proportional to the absolut value of this calculated error:\n",
    "\n",
    "$$P(i) = \\vert \\delta_{i} \\vert + \\epsilon$$\n",
    "\n",
    "When we sample experiences we can convert these priority weights of each tuple to a probability of chosing that tuple for the current batch by deviding each priority value by the total sum of all priority weights. However, greedy TD-error prioritization has several issues. It focuses on a small subset of experience and errors shrink slowly, especially when using function approximation, meaning that the initially high error transitions get replay frequently. This lack of diversity makes the system prone to overfitting. To overcome these issues, each priority is raised to a power of a priority scaling constant $\\alpha$. This leads to a stochastic sampling method, where $\\alpha$ equals one results in fully priority sampling and $\\alpha$ equals zero results in pure random sampling of normal experience replay:\n",
    "\n",
    "$$P_{r}(i) = \\frac{P_{i}^\\alpha} {\\sum_{k}{P_{k}^\\alpha}}$$\n",
    "\n",
    "Now we have taken care of the sampling of experience according to the priorities as well as setting the priorities of each sampled batch of experience tuples. But there is one more adjustment to make.\n",
    "\n",
    "**Importance sampling weight**\n",
    "\n",
    "Prioritized replay introduces bias towards those experiences, that have a higher priority, as we are now sampling experience non uniformly. To understand why, let's look at how we update the Q-network for each state-acton pair. For each optimization problem we first need to define a loss function that we want to minimize, which in our case is the squared TD-error:\n",
    "\n",
    "$$loss = \\vert \\delta^2 \\vert$$\n",
    "\n",
    "Then for each layer in the Q-network we update the weights and biases, which are represented through $\\theta$, towards the negativ gradient of the loss function and weight that by a learning rate $\\alpha$:\n",
    "\n",
    "$$\\theta = \\theta - \\alpha \\frac{dloss} {d\\theta}$$\n",
    "\n",
    "Ideally we do that for each state-action pair the environment gives us. With experience replay we are approximating the environment with the replay buffer and accelerating the training by sampling a batch of experience to train on. And because we are sampling randomly, the original distribution of experiences that the environment produces is preserved. However, once we introduce sampling priorities to the replay buffer, this changes the distribution if favor of the higher prioritized experiences, making the network anticipate these experiences more and overfit them. \n",
    "\n",
    "We can correct this bias by using importance sampling weights that fully compensates for the non-uniform probabilities $P(i)$ if $\\beta=1$:\n",
    "\n",
    "$$w_{i} = \\left(\\frac{1} N \\cdot \\frac{1} {P_{r}(i)}\\right)^\\beta$$\n",
    "\n",
    "The value of the weight factor $w_{i}$ is one devided by the total replay buffer size, times one devided by the probability. This will reduce the step size for higher probabilitiy experiences and therefore avoid excessiv update steps from the increased frequency of training on these experiences. Since the unbiased nature ofthe updates is most important near convergence at the end of training, as the processs is highly non-stationary anyay, we can introduce a schedule on the exponent $\\beta$ that reaches 1 only at the end of learning. In practice, we linnearly anneal $\\beta$ rom its initial value $\\beta_{0}$ to 1. For stability reasons, we can also normalize weights by $\\frac{1} {\\max_{i} w_{i}}$, so that they only scale the update downwoards.\n",
    "\n",
    "These importance sampling weights can be folded into into the Q-learning update by using $w_{i} \\delta_{i}$ instead of $\\delta_{i}$:\n",
    "\n",
    "$$\\theta = \\theta - \\alpha \\cdot \\frac{dloss} {d\\theta} w_{i}$$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
